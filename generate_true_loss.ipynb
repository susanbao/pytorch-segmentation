{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ef5a8f-c87e-41d0-be66-cc47746c22f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from utils import losses\n",
    "import torch\n",
    "from utils.utils import *\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c9864f-300e-47b6-8cf7-30a4f0085023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PSPNet_VOC, UNet_COCO10k, UNet_VOC\n",
    "base_path = \"./pro_data/UNet_VOC/train/\"\n",
    "files = os.listdir(base_path + \"output\")\n",
    "cross_entropy_loss_func = losses.CrossEntropyLoss2d(reduction=\"none\")\n",
    "file_count = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94e2f7d-c9da-451b-b36b-dd4680133952",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "check_folder_exist(base_path + \"loss\")\n",
    "for file in range(file_count):\n",
    "    output = np_read(base_path + f\"output/{file}.npy\")\n",
    "    target = np_read(base_path + f\"target/{file}.npy\")\n",
    "    losses = cross_entropy_loss_func(torch.from_numpy(output), torch.from_numpy(target))\n",
    "    losses = losses.numpy()\n",
    "    np_write(losses, base_path + f\"loss/{file}.npy\")\n",
    "    if (file+1)%100 == 0:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05356ae8-0b0d-4dee-a541-425cc9789f42",
   "metadata": {},
   "source": [
    "## Generate ground truth image-based loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19cff7cb-e47f-4c42-9ebb-fdb1e6e41932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "base_path = \"./pro_data/UNet_VOC/train/\"\n",
    "files = os.listdir(base_path + \"loss\")\n",
    "file_count = len(files)\n",
    "true_losses = None\n",
    "for file in range(file_count):\n",
    "    losses = np_read(base_path + f\"loss/{file}.npy\")\n",
    "    losses = np.reshape(losses, (losses.shape[0], -1))\n",
    "    losses = np.mean(losses, axis=1)\n",
    "    if true_losses is None:\n",
    "        true_losses = losses\n",
    "    else:\n",
    "        true_losses = np.concatenate((true_losses, losses), axis=0)\n",
    "    if (file+1)%100 == 0:\n",
    "        print(file)\n",
    "np_write(true_losses, base_path + \"image_true_losses.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a43e3-cd17-4699-8c08-6475b6d544e9",
   "metadata": {},
   "source": [
    "## Generate ground truth region-baed loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bc9eb08-2fe9-4a2f-ac77-2104134a971c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "## region 8x8\n",
    "import torch\n",
    "# base_path = \"./pro_data/UNet_VOC/val/\"\n",
    "files = os.listdir(base_path + \"output\")\n",
    "file_count = len(files)\n",
    "avgpool = torch.nn.AdaptiveAvgPool2d((60,60))\n",
    "region_based_losses = None\n",
    "for file in range(file_count):\n",
    "    losses = np_read(base_path + f\"loss/{file}.npy\")\n",
    "    losses = torch.from_numpy(losses)\n",
    "    losses = avgpool(losses)\n",
    "    losses = losses.numpy()\n",
    "    losses = losses.reshape(-1)\n",
    "    if region_based_losses is None:\n",
    "        region_based_losses = losses\n",
    "    else:\n",
    "        region_based_losses = np.concatenate((region_based_losses, losses), axis=0)\n",
    "    if (file+1)%100 == 0:\n",
    "        print(file)\n",
    "np_write(region_based_losses, base_path + \"region_8_8_true_losses.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2fb4feb-5741-46f2-b27f-105f49c26ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "## region 16x16\n",
    "import torch\n",
    "# base_path = \"./pro_data/UNet_VOC/val/\"\n",
    "files = os.listdir(base_path + \"output\")\n",
    "file_count = len(files)\n",
    "avgpool = torch.nn.AdaptiveAvgPool2d((30,30))\n",
    "region_based_losses = None\n",
    "for file in range(file_count):\n",
    "    losses = np_read(base_path + f\"loss/{file}.npy\")\n",
    "    losses = torch.from_numpy(losses)\n",
    "    losses = avgpool(losses)\n",
    "    losses = losses.numpy()\n",
    "    losses = losses.reshape(-1)\n",
    "    if region_based_losses is None:\n",
    "        region_based_losses = losses\n",
    "    else:\n",
    "        region_based_losses = np.concatenate((region_based_losses, losses), axis=0)\n",
    "    if (file+1)%100 == 0:\n",
    "        print(file)\n",
    "np_write(region_based_losses, base_path + \"region_16_16_true_losses.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ea7cb-5132-4be9-9237-1d813114b343",
   "metadata": {},
   "source": [
    "## Entropy part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0729e87b-064d-47c8-80e2-0c0a5c2fad5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "# PSPNet_VOC, UNet_COCO10k, UNet_VOC\n",
    "# base_path = \"./pro_data/UNet_VOC/val/\"\n",
    "files = os.listdir(base_path + \"output\")\n",
    "file_count = len(files)\n",
    "check_folder_exist(base_path + \"entropy\")\n",
    "for file in range(file_count):\n",
    "    output = np_read(base_path + f\"output/{file}.npy\")\n",
    "    output = torch.from_numpy(output)\n",
    "    output = F.softmax(output, dim=1)\n",
    "    entropy = torch.sum(torch.mul(-output, torch.log(output + 1e-20)), dim=1).unsqueeze(dim=1)\n",
    "    np_write(entropy.numpy(), base_path + f\"entropy/{file}.npy\")\n",
    "    if (file+1)%100 == 0:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff966ad-e66e-4947-a957-e2d986ea4626",
   "metadata": {},
   "source": [
    "## Feature Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f7c5fd4-e00a-4f1e-afef-5b9c12018ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "# PSPNet_VOC, UNet_COCO10k, UNet_VOC\n",
    "# base_path = \"./pro_data/UNet_VOC/val/\"\n",
    "files = os.listdir(base_path + \"output\")\n",
    "file_count = len(files)\n",
    "check_folder_exist(base_path + \"feature\")\n",
    "for file in range(file_count):\n",
    "    output = np_read(base_path + f\"output/{file}.npy\")\n",
    "    output = torch.from_numpy(output)\n",
    "    output = F.softmax(output, dim=1)\n",
    "    np_write(output.numpy(), base_path + f\"feature/{file}.npy\")\n",
    "    if (file+1)%100 == 0:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e353281-f3b5-4e40-b494-e1a9c03ab639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "199\n",
      "299\n",
      "399\n",
      "499\n",
      "599\n",
      "699\n",
      "799\n",
      "899\n",
      "999\n",
      "1099\n"
     ]
    }
   ],
   "source": [
    "## split feature\n",
    "base_path = \"./pro_data/UNet_COCO10k/train/\"\n",
    "check_folder_exist(base_path + \"feature_split\")\n",
    "files = os.listdir(base_path + \"feature\")\n",
    "file_count = len(files)\n",
    "count = 0\n",
    "for file in range(file_count):\n",
    "    feature = np_read(base_path + f\"feature/{file}.npy\")\n",
    "    samples = feature.shape[0]\n",
    "    for i in range(samples):\n",
    "        np_write(feature[i], base_path + f\"feature_split/{count}.npy\")\n",
    "        count += 1\n",
    "    if (file+1)%100 == 0:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d7af5-495a-41ed-8a6e-96ae42271057",
   "metadata": {},
   "source": [
    "## Check loss range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ffc19e-51be-49c8-92c3-c2c837ca6f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.087222606 9.087774\n",
      "0.0 18.521416\n",
      "0.0 20.193636\n"
     ]
    }
   ],
   "source": [
    "base_path = \"./pro_data/UNet_COCO10k/val/\"\n",
    "image_true_losses = np_read(base_path + \"image_true_losses.npy\")\n",
    "print(image_true_losses.min(), image_true_losses.max())\n",
    "region_16_16_true_losses = np_read(base_path + \"region_16_16_true_losses.npy\")\n",
    "print(region_16_16_true_losses.min(), region_16_16_true_losses.max())\n",
    "region_8_8_true_losses = np_read(base_path + \"region_8_8_true_losses.npy\")\n",
    "print(region_8_8_true_losses.min(), region_8_8_true_losses.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a81323-2f0a-4089-86ca-3a73d2745674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 7.2167935\n",
      "0.0 21.743809\n",
      "0.0 23.96487\n"
     ]
    }
   ],
   "source": [
    "base_path = \"./pro_data/UNet_COCO10k/train/\"\n",
    "image_true_losses = np_read(base_path + \"image_true_losses.npy\")\n",
    "print(image_true_losses.min(), image_true_losses.max())\n",
    "region_16_16_true_losses = np_read(base_path + \"region_16_16_true_losses.npy\")\n",
    "print(region_16_16_true_losses.min(), region_16_16_true_losses.max())\n",
    "region_8_8_true_losses = np_read(base_path + \"region_8_8_true_losses.npy\")\n",
    "print(region_8_8_true_losses.min(), region_8_8_true_losses.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf228a85-91ae-43b7-916f-ab23a9f20e6b",
   "metadata": {},
   "source": [
    "## Generate 4 splited loss ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b4995fc-dd42-4047-aaf2-20b6e8b69480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = \"./pro_data/PSPNet_VOC/val/\"\n",
    "files = os.listdir(base_path + \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "101b60ae-9cd5-4c5d-af20-6ce77e1228a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "avgpool = torch.nn.AdaptiveAvgPool2d((2,2))\n",
    "region_based_losses = None\n",
    "for file in files:\n",
    "    losses = np_read(base_path + f\"loss/{file}\")\n",
    "    losses = torch.from_numpy(losses)\n",
    "    losses = avgpool(losses)\n",
    "    losses = losses.numpy()\n",
    "    losses = losses.reshape(-1)\n",
    "    if region_based_losses is None:\n",
    "        region_based_losses = losses\n",
    "    else:\n",
    "        region_based_losses = np.concatenate((region_based_losses, losses), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8532c7d3-98b7-49c8-95b1-0408e2d586c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np_write(region_based_losses, base_path + \"image_split_2_2_true_losses.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c337f51c-023d-4940-a345-37ac60e6f81e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0793692, 0.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_based_losses.max(), region_based_losses.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68e2fe-519b-4ba4-9614-4f716ed82995",
   "metadata": {},
   "source": [
    "## Generate 8x8 patch ground truth for 4 splited image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b13ce5bb-29b6-4af5-98f5-3292f8d801f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = \"./pro_data/PSPNet_VOC/val/\"\n",
    "files = os.listdir(base_path + \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e203945b-f5d0-4c83-a271-d7752a74e7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "avgpool = torch.nn.AdaptiveAvgPool2d((60,60))\n",
    "splited_location_list = [[0,30,0,30], [0,30,30,60],[30,60,0,30],[30,60,30,60]]\n",
    "region_based_losses = None\n",
    "for file in files:\n",
    "    losses = np_read(base_path + f\"loss/{file}\")\n",
    "    losses = torch.from_numpy(losses)\n",
    "    losses = avgpool(losses)\n",
    "    losses = losses.numpy()\n",
    "    for i in range(losses.shape[0]):\n",
    "        for j in range(4):\n",
    "            location = splited_location_list[j]\n",
    "            loss = losses[i, location[0]:location[1],location[2]:location[3]]\n",
    "            loss = loss.reshape(-1)\n",
    "            if region_based_losses is None:\n",
    "                region_based_losses = loss\n",
    "            else:\n",
    "                region_based_losses = np.concatenate((region_based_losses, loss), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe520ea3-034f-44ae-b7e2-3b6393a9e3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np_write(region_based_losses, base_path + \"region_8_8_image_split_true_losses.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0fe99bd-6e0e-4445-8b2a-e220625a85d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5216400,), 10.661509, 0.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_based_losses.shape, region_based_losses.max(), region_based_losses.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79168644-4b31-433f-a6db-da18876b3e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
