{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141c9d4d-3624-496f-a35f-6e4779669dcb",
   "metadata": {},
   "source": [
    "# Test different metric to find the best one to select top 900 hard patches in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a58e832-dd07-4617-ab4d-a05e51e414ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler, random_split, TensorDataset\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1df47a7-11ff-4c44-8866-5c000223973b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_one_results(path):\n",
    "    with open(path, \"r\") as outfile:\n",
    "        data = json.load(outfile)\n",
    "    return data\n",
    "\n",
    "def write_one_results(path, json_data):\n",
    "    with open(path, \"w\") as outfile:\n",
    "        json.dump(json_data, outfile)\n",
    "        \n",
    "def display_data_hist(loss):\n",
    "    plt.hist(loss, bins=50, label='ViT')\n",
    "    plt.title('Loss Distribution')\n",
    "    plt.xlabel('Loss')\n",
    "    plt.ylabel('Probability Density')\n",
    "    locs, _ = plt.yticks()\n",
    "    plt.yticks(locs,np.round(locs/loss.shape[0],3))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def np_read(file):\n",
    "    with open(file, \"rb\") as outfile:\n",
    "        data = np.load(outfile)\n",
    "    return data\n",
    "def np_write(data, file):\n",
    "    with open(file, \"wb\") as outfile:\n",
    "        np.save(outfile, data)\n",
    "        \n",
    "def compute_kl_divergence(class_prob_1, class_prob_2):\n",
    "    kl_divergence = torch.sum(class_prob_1 * (torch.log(class_prob_1) - torch.log(class_prob_2)), dim=0)\n",
    "    return kl_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ca207-d468-4a63-8685-75abc8a4fb2d",
   "metadata": {},
   "source": [
    "## Inconsistency-based metric\n",
    "Not All Labels Are Equal: Rationalizing The Labeling Costs for Training Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4e1c5694-5c0c-4e64-a27f-13a41cd2148a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "base_path = \"/workspace/pytorch-segmentation/pro_data/PSPNet_VOC/\" + split + \"/\"\n",
    "output_path = base_path + \"output/\"\n",
    "label_path = base_path + \"target/\"\n",
    "hflip_output_path = base_path + \"hflip_output/\"\n",
    "true_loss = np_read(base_path + \"region_8_8_true_losses.npy\")\n",
    "avgpool = torch.nn.AdaptiveAvgPool2d((60,60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "61cdae6c-b6f2-45f3-a6a7-cdeacec59449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_patch_kl(index):\n",
    "    image_index = index % 8\n",
    "    file_name = str(index//8) + \".npy\"\n",
    "    output = torch.from_numpy(np_read(output_path + file_name))\n",
    "    hflip_output = torch.from_numpy(np_read(hflip_output_path + file_name))\n",
    "    output = output[image_index]\n",
    "    hflip_output = hflip_output[image_index]\n",
    "    hflip_output = torch.flip(hflip_output, dims=[2])\n",
    "    output = F.softmax(output, dim=0)\n",
    "    hflip_output = F.softmax(hflip_output, dim=0)\n",
    "    kl = compute_kl_divergence(output, hflip_output) + compute_kl_divergence(hflip_output, output)\n",
    "    kl = kl.unsqueeze(dim=0)\n",
    "    patch_kl = avgpool(kl).squeeze(dim=0)\n",
    "    return patch_kl\n",
    "\n",
    "def get_probability(index):\n",
    "    image_index = index % 8\n",
    "    file_name = str(index//8) + \".npy\"\n",
    "    output = torch.from_numpy(np_read(output_path + file_name))\n",
    "    output = output[image_index]\n",
    "    output = F.softmax(output, dim=0)\n",
    "    return output\n",
    "\n",
    "def get_hflip_probability(index):\n",
    "    image_index = index % 8\n",
    "    file_name = str(index//8) + \".npy\"\n",
    "    output = torch.from_numpy(np_read(hflip_output_path + file_name))\n",
    "    output = output[image_index]\n",
    "    output = torch.flip(output, dims=[2])\n",
    "    output = F.softmax(output, dim=0)\n",
    "    return output\n",
    "\n",
    "def get_label(index):\n",
    "    image_index = index % 8\n",
    "    file_name = str(index//8) + \".npy\"\n",
    "    output = torch.from_numpy(np_read(label_path + file_name))\n",
    "    output = output[image_index]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c0978763-5a34-4b68-8a67-dd0631db3e36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  88,   88,  208, 1446, 1446, 1446, 1446, 1446]),\n",
       " array([1266, 1325, 2097,  647,  710,  711,  770,  833]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analysis\n",
    "larger_loc = np.where(true_loss > 9)[0]\n",
    "larger_loc // 3600, larger_loc % 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "704820f9-8b30-4191-bb38-d8da4b85cfcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.258706092834473, 0.00010118322825292125\n",
      "9.76128101348877, 0.00013450768892653286\n",
      "10.374043464660645, 0.0001369493402307853\n",
      "9.936532020568848, 4.634981451090425e-05\n",
      "10.661508560180664, 9.418664558324963e-05\n"
     ]
    }
   ],
   "source": [
    "index = 1446\n",
    "image_patch_true_loss = true_loss[index*patch_pre_image: (index+1)*patch_pre_image]\n",
    "image_patch_kl = compute_patch_kl(index).reshape(-1).numpy()\n",
    "prob = get_probability(index)\n",
    "label = get_label(index)\n",
    "hflip_prob = get_hflip_probability(index)\n",
    "bad_case_index = [647,  710,  711,  770,  833]\n",
    "for i in bad_case_index:\n",
    "    print(f\"{image_patch_true_loss[i]}, {image_patch_kl[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c520aa9c-7d56-48f0-941a-f700b73e01d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patch_index = 647\n",
    "h = patch_index // 60\n",
    "w = patch_index % 60\n",
    "patch_prob = prob[:,h*8:(h+1)*8, w*8:(w+1)*8]\n",
    "patch_label = label[h*8:(h+1)*8, w*8:(w+1)*8]\n",
    "patch_hflip_prob = hflip_prob[:,h*8:(h+1)*8, w*8:(w+1)*8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0ddbf482-0bca-4968-9faa-c5654dfe133f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability:\n",
      "tensor([9.9982e-01, 2.4129e-06, 6.6885e-07, 1.6410e-06, 2.2348e-07, 3.2198e-05,\n",
      "        6.1785e-07, 1.7244e-07, 2.5458e-07, 2.3559e-06, 2.7047e-07, 1.1101e-06,\n",
      "        2.4424e-07, 6.3243e-07, 2.6687e-07, 1.2240e-04, 1.1638e-05, 1.8349e-07,\n",
      "        3.4116e-07, 3.6509e-06, 2.3800e-06])\n",
      "label:\n",
      "tensor(5)\n",
      "flip probability:\n",
      "tensor([9.9976e-01, 6.8800e-06, 9.0307e-07, 2.5657e-06, 4.8761e-07, 3.3633e-05,\n",
      "        1.6098e-06, 3.9884e-07, 4.7293e-07, 1.3535e-05, 4.3102e-07, 2.1768e-06,\n",
      "        4.9448e-07, 9.9665e-07, 1.2732e-07, 1.4802e-04, 1.9782e-05, 3.4414e-07,\n",
      "        8.3785e-07, 5.2300e-06, 2.7343e-06])\n"
     ]
    }
   ],
   "source": [
    "h_index = 7\n",
    "w_index = 7\n",
    "print(\"probability:\")\n",
    "print(patch_prob[:,h_index,w_index])\n",
    "print(\"label:\")\n",
    "print(patch_label[h_index,w_index])\n",
    "print(\"flip probability:\")\n",
    "print(patch_hflip_prob[:,h_index,w_index])\n",
    "# patch_prob[:,h_index,w_index], patch_label[h_index,w_index], patch_hflip_prob[:,h_index,w_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80856d97-734c-469a-a650-f0062370bca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patch_pre_image = 3600\n",
    "choose_patch_num = 900\n",
    "image_nums = true_loss.size // patch_pre_image\n",
    "pre_find_patch = np.zeros(image_nums)\n",
    "for index in range(image_nums):\n",
    "    image_patch_true_loss = true_loss[index*patch_pre_image: (index+1)*patch_pre_image]\n",
    "    image_patch_kl = compute_patch_kl(index).reshape(-1).numpy()\n",
    "    top_k_true_loss_index = np.sort(np.argsort(image_patch_true_loss)[-choose_patch_num:])\n",
    "    top_k_patch_kl = np.sort(np.argsort(image_patch_kl)[-choose_patch_num:])\n",
    "    common_integers = np.intersect1d(top_k_true_loss_index, top_k_patch_kl)\n",
    "    pre_find_patch[index] = common_integers.size / choose_patch_num\n",
    "print(f\"{pre_find_patch.mean()} percentage of top {choose_patch_num} have been selected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "146c3f23-1b2e-4e7b-acb8-c4f7fc585bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 99\n",
      "Process 199\n",
      "Process 299\n",
      "Process 399\n",
      "Process 499\n",
      "Process 599\n",
      "Process 699\n",
      "Process 799\n",
      "Process 899\n",
      "Process 999\n",
      "Process 1099\n",
      "Process 1199\n",
      "Process 1299\n",
      "Process 1399\n",
      "Threshold: 1, found percetage: 0.7428618747929778\n",
      "Threshold: 2, found percetage: 0.654007507175977\n",
      "Threshold: 3, found percetage: 0.561374549819928\n",
      "Threshold: 4, found percetage: 0.5047281323877069\n",
      "Threshold: 5, found percetage: 0.45728155339805826\n",
      "Threshold: 6, found percetage: 0.4068627450980392\n",
      "Threshold: 7, found percetage: 0.3384615384615385\n",
      "Threshold: 8, found percetage: 0.32432432432432434\n",
      "Threshold: 9, found percetage: 0.0\n"
     ]
    }
   ],
   "source": [
    "threshold_list = [1,2,3,4,5,6,7,8,9]\n",
    "and_count_list = np.zeros(len(threshold_list))\n",
    "large_thre_count_list = np.zeros(len(threshold_list))\n",
    "patch_pre_image = 3600\n",
    "choose_patch_num = 900\n",
    "image_nums = true_loss.size // patch_pre_image\n",
    "for index in range(image_nums):\n",
    "    image_patch_true_loss = true_loss[index*patch_pre_image: (index+1)*patch_pre_image]\n",
    "    image_patch_kl = compute_patch_kl(index).reshape(-1).numpy()\n",
    "    top_k_patch_kl = np.sort(np.argsort(image_patch_kl)[-choose_patch_num:])\n",
    "    top_k_patch_kl_bool = np.full(shape=image_patch_kl.shape, fill_value=False, dtype=bool)\n",
    "    top_k_patch_kl_bool[top_k_patch_kl] = True\n",
    "\n",
    "    for i in range(len(threshold_list)):\n",
    "        bool_large_thre = image_patch_true_loss > threshold_list[i]\n",
    "        and_results = np.logical_and(bool_large_thre, top_k_patch_kl_bool)\n",
    "        and_count_list[i] += and_results.sum()\n",
    "        large_thre_count_list[i] += bool_large_thre.sum()\n",
    "    if (index + 1) % 100 == 0:\n",
    "        print(f\"Process {index}\")\n",
    "percentage = and_count_list / large_thre_count_list\n",
    "for i in range(len(threshold_list)):\n",
    "    print(f\"Threshold: {threshold_list[i]}, found percetage: {percentage[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bf62bc64-d96b-4aa1-9670-7e4b87058470",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 99\n",
      "Process 199\n",
      "Process 299\n",
      "Process 399\n",
      "Process 499\n",
      "Process 599\n",
      "Process 699\n",
      "Process 799\n",
      "Process 899\n",
      "Process 999\n",
      "Process 1099\n",
      "Process 1199\n",
      "Process 1299\n",
      "Process 1399\n",
      "Threshold: 1, found percetage: 0.7442199403776085\n",
      "Threshold: 2, found percetage: 0.6468867299624641\n",
      "Threshold: 3, found percetage: 0.546968787515006\n",
      "Threshold: 4, found percetage: 0.48857368006304175\n",
      "Threshold: 5, found percetage: 0.4300970873786408\n",
      "Threshold: 6, found percetage: 0.3897058823529412\n",
      "Threshold: 7, found percetage: 0.3230769230769231\n",
      "Threshold: 8, found percetage: 0.2972972972972973\n",
      "Threshold: 9, found percetage: 0.0\n"
     ]
    }
   ],
   "source": [
    "# maxpool\n",
    "maxpool = torch.nn.AdaptiveMaxPool2d((60,60))\n",
    "def compute_patch_kl(index):\n",
    "    image_index = index % 8\n",
    "    file_name = str(index//8) + \".npy\"\n",
    "    output = torch.from_numpy(np_read(output_path + file_name))\n",
    "    hflip_output = torch.from_numpy(np_read(hflip_output_path + file_name))\n",
    "    output = output[image_index]\n",
    "    hflip_output = hflip_output[image_index]\n",
    "    hflip_output = torch.flip(hflip_output, dims=[2])\n",
    "    output = F.softmax(output, dim=0)\n",
    "    hflip_output = F.softmax(hflip_output, dim=0)\n",
    "    kl = compute_kl_divergence(output, hflip_output) + compute_kl_divergence(hflip_output, output)\n",
    "    kl = kl.unsqueeze(dim=0)\n",
    "    patch_kl = maxpool(kl).squeeze(dim=0)\n",
    "    return patch_kl\n",
    "\n",
    "threshold_list = [1,2,3,4,5,6,7,8,9]\n",
    "and_count_list = np.zeros(len(threshold_list))\n",
    "large_thre_count_list = np.zeros(len(threshold_list))\n",
    "patch_pre_image = 3600\n",
    "choose_patch_num = 900\n",
    "image_nums = true_loss.size // patch_pre_image\n",
    "for index in range(image_nums):\n",
    "    image_patch_true_loss = true_loss[index*patch_pre_image: (index+1)*patch_pre_image]\n",
    "    image_patch_kl = compute_patch_kl(index).reshape(-1).numpy()\n",
    "    top_k_patch_kl = np.sort(np.argsort(image_patch_kl)[-choose_patch_num:])\n",
    "    top_k_patch_kl_bool = np.full(shape=image_patch_kl.shape, fill_value=False, dtype=bool)\n",
    "    top_k_patch_kl_bool[top_k_patch_kl] = True\n",
    "\n",
    "    for i in range(len(threshold_list)):\n",
    "        bool_large_thre = image_patch_true_loss > threshold_list[i]\n",
    "        and_results = np.logical_and(bool_large_thre, top_k_patch_kl_bool)\n",
    "        and_count_list[i] += and_results.sum()\n",
    "        large_thre_count_list[i] += bool_large_thre.sum()\n",
    "    if (index + 1) % 100 == 0:\n",
    "        print(f\"Process {index}\")\n",
    "percentage = and_count_list / large_thre_count_list\n",
    "for i in range(len(threshold_list)):\n",
    "    print(f\"Threshold: {threshold_list[i]}, found percetage: {percentage[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108c183-1a12-4557-bbc7-f46af7a726fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0a492bff-8997-43b9-a3dd-3a2cb3653e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_estimated_loss = np.array(read_one_results(\"/workspace/ViT-pytorch/output/region_8_8/ViT-output-PSPNet-VOC-train-ordinal-8x8-patch_losses_9800.json\")['losses'])\n",
    "threshold_list = [1,2,3,4,5,6,7,8,9]\n",
    "and_count_list = np.zeros(len(threshold_list))\n",
    "large_thre_count_list = np.zeros(len(threshold_list))\n",
    "top_k_patch_kl_bool = val_estimated_loss > 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "61df4607-e9fc-486c-bbbd-4bb5f34acbde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 1, found percetage: 0.8298111957601855\n",
      "Threshold: 2, found percetage: 0.6811658202693751\n",
      "Threshold: 3, found percetage: 0.5571728691476591\n",
      "Threshold: 4, found percetage: 0.4578408195429472\n",
      "Threshold: 5, found percetage: 0.3533980582524272\n",
      "Threshold: 6, found percetage: 0.4019607843137255\n",
      "Threshold: 7, found percetage: 0.5384615384615384\n",
      "Threshold: 8, found percetage: 0.6486486486486487\n",
      "Threshold: 9, found percetage: 0.125\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(threshold_list)):\n",
    "    bool_large_thre = true_loss > threshold_list[i]\n",
    "    and_results = np.logical_and(bool_large_thre, top_k_patch_kl_bool)\n",
    "    and_count_list[i] += and_results.sum()\n",
    "    large_thre_count_list[i] += bool_large_thre.sum()\n",
    "percentage = and_count_list / large_thre_count_list\n",
    "for i in range(len(threshold_list)):\n",
    "    print(f\"Threshold: {threshold_list[i]}, found percetage: {percentage[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e46d5-178c-4475-9fbf-7198b1f529ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
