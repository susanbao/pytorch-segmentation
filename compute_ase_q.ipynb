{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e6a0aa-10b8-4517-88e2-5eed81358c23",
   "metadata": {},
   "source": [
    "# Compute the q value for ASE method\n",
    "The active testing can use these q values. Use uncertainty quantification method: deep ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f4ff4-31ed-4e22-b26e-ba757da80f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference code\n",
    "class SelfSurrogateAcquisitionSurrogateWeightedBALD2(\n",
    "        _SelfSurrogateAcquisitionBase):\n",
    "\n",
    "    def __init__(self, cfg, dataset, model, model_cfg, *args, **kwargs):\n",
    "        super().__init__(cfg, dataset, model, model_cfg)\n",
    "\n",
    "    def expected_loss(self, remaining_data, remaining_idxs):\n",
    "\n",
    "        eps = 1e-19\n",
    "        model_pred = self.model.predict(remaining_data, remaining_idxs)\n",
    "        model_pred = np.clip(model_pred, eps, 1 - eps)\n",
    "        model_pred /= model_pred.sum(axis=1, keepdims=True)\n",
    "\n",
    "        surr_pred = np.exp(self.surr_model.joint_predict(\n",
    "            remaining_data, remaining_idxs))\n",
    "        surr_mean_pred = surr_pred.mean(0)\n",
    "        weights = - np.log(model_pred)\n",
    "\n",
    "        # sum over classes to get entropy\n",
    "        entropy_average = - weights * surr_mean_pred * np.log(surr_mean_pred)\n",
    "        entropy_average = entropy_average.sum(-1)\n",
    "\n",
    "        # these are all probabilities\n",
    "        # N_ensemble x N_data x N_classes\n",
    "        weights = weights[np.newaxis, ...]\n",
    "        average_entropy = - weights * surr_pred * np.log(surr_pred)\n",
    "        average_entropy = np.sum(average_entropy, -1)\n",
    "        average_entropy = np.mean(average_entropy, 0)\n",
    "\n",
    "        bald = entropy_average - average_entropy\n",
    "\n",
    "        return bald\n",
    "\n",
    "class AnySurrogateAcquisitionEntropy(\n",
    "        _AnySurrogateAcquisitionBase):\n",
    "\n",
    "    def __init__(self, cfg, dataset, model, model_cfg, *args, **kwargs):\n",
    "        super().__init__(cfg, dataset, model, model_cfg)\n",
    "\n",
    "    def expected_loss(self, remaining_data, remaining_idxs):\n",
    "        return entropy_loss(\n",
    "            remaining_data, remaining_idxs, self.model, self.surr_model,\n",
    "            cfg=self.cfg)\n",
    "\n",
    "def entropy_loss(\n",
    "        remaining_data, remaining_idxs, model, surr_model=None,\n",
    "        eps=1e-15, T=None, cfg=None, extra_log=False):\n",
    "\n",
    "    model_pred = model.predict(remaining_data, idxs=remaining_idxs)\n",
    "\n",
    "    if T is not None:\n",
    "        model_pred = np.exp(np.log(model_pred)/T)\n",
    "\n",
    "        model_pred = np.clip(model_pred, eps, 1/eps)\n",
    "        model_pred[np.isnan(model_pred)] = 1/eps\n",
    "\n",
    "        model_pred /= model_pred.sum(axis=1, keepdims=True)\n",
    "\n",
    "        model_pred = np.clip(model_pred, eps, 1/eps)\n",
    "        model_pred[np.isnan(model_pred)] = 1/eps\n",
    "\n",
    "    if surr_model is not None:\n",
    "        surr_model_pred = surr_model.predict(\n",
    "            remaining_data, idxs=remaining_idxs)\n",
    "\n",
    "        if T is not None:\n",
    "            surr_model_pred = np.exp(np.log(surr_model_pred)/T)\n",
    "            surr_model_pred = np.clip(surr_model_pred, eps, 1/eps)\n",
    "            surr_model_pred[np.isnan(surr_model_pred)] = 1/eps\n",
    "\n",
    "            surr_model_pred /= surr_model_pred.sum(axis=1, keepdims=True)\n",
    "            surr_model_pred = np.clip(surr_model_pred, eps, 1/eps)\n",
    "            surr_model_pred[np.isnan(surr_model_pred)] = 1/eps\n",
    "\n",
    "    else:\n",
    "        surr_model_pred = model_pred\n",
    "\n",
    "    if T is None:\n",
    "        model_pred = np.clip(model_pred, eps, 1 - eps)\n",
    "        model_pred /= model_pred.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Sum_{y=c} p_surr(y=c|x) log p_model(y=c|x)\n",
    "    if not extra_log:\n",
    "        res = -1 * (surr_model_pred * np.log(model_pred)).sum(-1)\n",
    "    else:\n",
    "        raise NotImplementedError('Not sure what this should look like')\n",
    "        res = -1 * (surr_model_pred * np.log(model_pred)).sum(-1)\n",
    "\n",
    "    if T is not None:\n",
    "        res[np.isnan(res)] = np.nanmax(res)\n",
    "\n",
    "    if cfg is not None and not cfg.get('uniform_clip', False):\n",
    "        clip_val = np.percentile(res, 10)\n",
    "        res = np.clip(res, clip_val, 1/eps)\n",
    "\n",
    "    # clipping has moved to after acquisition\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002abfb3-5fee-4bc8-b586-e524f6f3be28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "import io\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from scipy.special import softmax\n",
    "from utils.utils import *\n",
    "\n",
    "# def read_one_results(path):\n",
    "#     with open(path, \"r\") as outfile:\n",
    "#         data = json.load(outfile)\n",
    "#     return data\n",
    "\n",
    "# def write_one_results(json_data, path):\n",
    "#     with open(path, \"w\") as outfile:\n",
    "#         json.dump(json_data, outfile)\n",
    "\n",
    "# def np_read(file):\n",
    "#     with open(file, \"rb\") as outfile:\n",
    "#         data = np.load(outfile)\n",
    "#     return data\n",
    "\n",
    "# def np_write(data, file):\n",
    "#     with open(file, \"wb\") as outfile:\n",
    "#         np.save(outfile, data)\n",
    "\n",
    "# def check_path_exist(path):\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782c7641-d6c9-4583-a5f4-00b655b68244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_folder_exist(f\"./ase_results/{data_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0d09b8-b544-4aa8-9b84-08ee954a0329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_type = \"PSPNet_VOC\"\n",
    "base_path = f\"./pro_data/{data_type}_ASE/\"\n",
    "output_list = [base_path + str(i) for i in range(1,5)]\n",
    "ego_output_path =  base_path + \"0\"\n",
    "file_list = os.listdir(ego_output_path)\n",
    "# file_num = len(file_list)\n",
    "file_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5356529c-e260-4542-a28a-7583b4441b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avgpool_32 = torch.nn.AdaptiveAvgPool2d((15,15))\n",
    "avgpool_16 = torch.nn.AdaptiveAvgPool2d((30,30))\n",
    "avgpool_8 = torch.nn.AdaptiveAvgPool2d((60,60))\n",
    "\n",
    "at_image_losses = None\n",
    "at_region_32_losses = None\n",
    "at_region_16_losses = None\n",
    "at_region_8_losses = None\n",
    "\n",
    "ase_image_losses = None\n",
    "ase_region_32_losses = None\n",
    "ase_region_16_losses = None\n",
    "ase_region_8_losses = None\n",
    "\n",
    "def average_pool(array, avgpool):\n",
    "    tensor = torch.from_numpy(array)\n",
    "    tensor = avgpool(tensor)\n",
    "    return tensor.numpy()\n",
    "\n",
    "def con_losses(losses, loss):\n",
    "    if losses is None:\n",
    "        return loss\n",
    "    else:\n",
    "        return np.concatenate((losses, loss))\n",
    "    \n",
    "def compute_active_testing_losses(ego_output, dropout_output):\n",
    "    global at_image_losses, at_region_32_losses, at_region_16_losses, at_region_8_losses\n",
    "    pi_output = dropout_output.mean(axis=0)\n",
    "    pi_output = pi_output / pi_output.sum(axis=1, keepdims=True)\n",
    "    at_loss = (-1 * (pi_output * np.log(ego_output))).sum(axis=1)\n",
    "    at_image_losses = con_losses(at_image_losses, np.mean(at_loss, axis=(1, 2)))\n",
    "    at_region_32_losses = con_losses(at_region_32_losses, average_pool(at_loss, avgpool_32).reshape(-1))\n",
    "    at_region_16_losses = con_losses(at_region_16_losses, average_pool(at_loss, avgpool_16).reshape(-1))\n",
    "    at_region_8_losses = con_losses(at_region_8_losses, average_pool(at_loss, avgpool_8).reshape(-1))\n",
    "    \n",
    "def compute_ase_bald2_losses(ego_output, dropout_output):\n",
    "    global ase_image_losses, ase_region_32_losses, ase_region_16_losses, ase_region_8_losses\n",
    "    mean_output = dropout_output.mean(axis=0)\n",
    "    mean_output = mean_output / mean_output.sum(axis=1, keepdims=True)\n",
    "    weights = -np.log(ego_output)\n",
    "    \n",
    "    # sum over classes to get entropy\n",
    "    entropy_average = - weights * mean_output * np.log(mean_output)\n",
    "    entropy_average = entropy_average.sum(axis=1)\n",
    "    \n",
    "    weights = weights[np.newaxis, ...]\n",
    "    average_entropy = - weights * dropout_output * np.log(dropout_output)\n",
    "    average_entropy = average_entropy.sum(axis=2)\n",
    "    average_entropy = average_entropy.mean(axis=0)\n",
    "    \n",
    "    bald = entropy_average - average_entropy\n",
    "    \n",
    "    ase_image_losses = con_losses(ase_image_losses, np.mean(bald, axis=(1, 2)))\n",
    "    ase_region_32_losses = con_losses(ase_region_32_losses, average_pool(bald, avgpool_32).reshape(-1))\n",
    "    ase_region_16_losses = con_losses(ase_region_16_losses, average_pool(bald, avgpool_16).reshape(-1))\n",
    "    ase_region_8_losses = con_losses(ase_region_8_losses, average_pool(bald, avgpool_8).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5085efc3-3078-4319-bb85-a66f87426f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'at_image_losses' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13161/1680999711.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdropout_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcompute_active_testing_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mego_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mcompute_ase_bald2_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mego_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13161/3319521129.py\u001b[0m in \u001b[0;36mcompute_active_testing_losses\u001b[0;34m(ego_output, dropout_output)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpi_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi_output\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpi_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mat_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpi_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mego_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mat_image_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcon_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat_image_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mat_region_32_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcon_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat_region_32_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavgpool_32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mat_region_16_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcon_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat_region_16_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavgpool_16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'at_image_losses' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for file in range(file_num):\n",
    "    ego_output = np_read(os.path.join(ego_output_path,f\"{file}.npy\"))\n",
    "    ego_output = softmax(ego_output)\n",
    "    dropout_output = np.expand_dims(np.copy(ego_output), axis=0)\n",
    "    for output_path in output_list:\n",
    "        temp_output = np_read(os.path.join(output_path,f\"{file}.npy\"))\n",
    "        temp_output = softmax(temp_output)\n",
    "        dropout_output = np.concatenate((dropout_output, np.expand_dims(np.copy(temp_output), axis=0)), axis=0)\n",
    "    \n",
    "    compute_active_testing_losses(ego_output, dropout_output)\n",
    "    compute_ase_bald2_losses(ego_output, dropout_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2491ee1-fd89-4a43-bcee-1092297146b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "store_path = \"./ase_results/\"\n",
    "np_write(at_image_losses, store_path + f\"AT_{data_type}_image_Q.npy\")\n",
    "np_write(at_region_32_losses, store_path + f\"AT_{data_type}_region_32_Q.npy\")\n",
    "np_write(at_region_16_losses, store_path + f\"AT_{data_type}_region_16_Q.npy\")\n",
    "np_write(at_region_8_losses, store_path + f\"AT_{data_type}_region_8_Q.npy\")\n",
    "\n",
    "np_write(ase_image_losses, store_path + f\"ASE_{data_type}_image_Q.npy\")\n",
    "np_write(ase_region_32_losses, store_path + f\"ASE_{data_type}_region_32_Q.npy\")\n",
    "np_write(ase_region_16_losses, store_path + f\"ASE_{data_type}_region_16_Q.npy\")\n",
    "np_write(ase_region_8_losses, store_path + f\"ASE_{data_type}_region_8_Q.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf2d5e7-4c3e-4ee0-b38d-e2f0ba165fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 0\n",
    "ego_output = np_read(os.path.join(ego_output_path,f\"{file}.npy\"))\n",
    "ego_output = softmax(ego_output)\n",
    "dropout_output = np.expand_dims(np.copy(ego_output), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ca9768-2026-4fcb-af19-537871917455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for output_path in output_list:\n",
    "    temp_output = np_read(os.path.join(output_path,f\"{file}.npy\"))\n",
    "    temp_output = softmax(temp_output)\n",
    "    dropout_output = np.concatenate((dropout_output, np.expand_dims(np.copy(temp_output), axis=0)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44d4f0bb-b61e-4d9f-b08f-d376aed634e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_active_testing_losses(ego_output, dropout_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9cb329e-5a64-4a5b-8ad8-0e0ba8359c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_ase_bald2_losses(ego_output, dropout_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa1130-af7d-4305-9ffa-acd9f66b8fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
